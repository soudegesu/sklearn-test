{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストを機械学習向けの表現に処理する前に、よくあるテキストデータの種類を把握する\n",
    "# 1. カテゴリデータ 2. 意味的にはカテゴリに分類できる自由に書かれた文字列 3 構造化された文字列 4 テキストデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n",
      "text_train[1]: b'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clich\\xc3\\xa9s, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\\'t list them here, but just mention the coloring of the plane. They didn\\'t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\\' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you\\'re choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.'\n"
     ]
    }
   ],
   "source": [
    "# サンプルデータは http://ai.stanford.edu/~amaas/data/sentiment/ から取得する\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files('data/aclImdb/train/')\n",
    "# load_files は一連の訓練テキストと訓練ラベルを返す\n",
    "text_train, y_train, = reviews_train.data, reviews_train.target\n",
    "print(f\"type of text_train: {type(text_train)}\")\n",
    "print(f\"length of text_train: {len(text_train)}\")\n",
    "print(f\"text_train[1]: {text_train[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "# 改行コードを削除する\n",
    "import numpy as np\n",
    "\n",
    "text_train = [doc.replace(b'<br />', b' ') for doc in text_train]\n",
    "print(f\"Samples per class (training): {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "# テストデータもロードする\n",
    "reviews_test = load_files('data/aclImdb/test/')\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(f\"Number of documents in test data: {len(text_test)}\")\n",
    "print(f\"Samples per class (test): {np.bincount(y_test)}\")\n",
    "text_test = [doc.replace(b'<br />', b' ') for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストデータに対して、「肯定的」または「否定的」のラベルをつける\n",
    "# BoWを使う\n",
    "# コーパスに対して、BoWを適用するには次の3ステップが必要になる\n",
    "# 1. トークン分割: 個々の文書（トークン）をホワイトスペースや句読点で区切る\n",
    "# 2. ボキャブラリ構築: すべての単語をボキャブラリとして集め、番号をつける\n",
    "# 3.エンコード: 個々の文書に対してボキャブラリの単語が現れる回数を数える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トイデータセットに対してBoWを適用してみる\n",
    "bards_word = ['The fool doth think he is wise,', 'but the wise man knows himself to be a fool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary content: {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_word)\n",
    "print(f\"Vocabulary size: {len(vect.vocabulary_)}\")\n",
    "print(f\"Vocabulary content: {vect.vocabulary_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_word)\n",
    "print(f\"bag_of_words: {repr(bag_of_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense representation of bag_of_words: [[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dense representation of bag_of_words: {bag_of_words.toarray()}\")\n",
    "# 出現回数がわかる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(f\"{repr(X_train)}\")\n",
    "# 74849個の単語から構成されていることがわかる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 74849\n",
      "First 20 features: ['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n",
      "Features 20010 to 20030: ['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl', 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\n",
      "Every 2000th feature: ['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful', 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher', 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse', 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"First 20 features: {feature_names[:20]}\")\n",
    "print(f\"Features 20010 to 20030: {feature_names[20010:20030]}\")\n",
    "print(f\"Every 2000th feature: {feature_names[::2000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数字が多いことがわかる。数字はデータ的に不要そうなので、省いて良いかもしれないが、007は映画の意味もあるため、そう簡単に見極められない\n",
    "# draも複数系の単語として意味が重複していることがあるので、別の特徴量としてカウントするのは理想的ではない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.88132\n"
     ]
    }
   ],
   "source": [
    "# 特徴量を改良する前に、クラス分類器を構築して、性能の定量的な指標を得ておこう\n",
    "# 訓練ラベルはy_trainに格納されており、訓練データのBoW表現はX_trainに格納されているので、クラス分類器を訓練することができる\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\n",
    "print(f\"Mean cross-validation accuracy: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.88816\n",
      "Best parameters: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "# 最良のパラメータを探す\n",
    "print(f\"Best cross-validation score: {grid.best_score_}\")\n",
    "print(f\"Best parameters: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87892\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(f\"{grid.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <25000x27271 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3354014 stored elements in Compressed Sparse Row format>\n",
      "First 50 features: ['00', '000', '007', '00s', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '1000', '100th', '101', '102', '103', '104', '105', '107', '108', '10s', '10th', '11', '110', '112', '116', '117', '11th', '12', '120', '12th', '13', '135', '13th', '14', '140', '14th', '15', '150', '15th', '16', '160', '1600', '16mm', '16s', '16th']\n",
      "Features 20010 to 20030: ['repentance', 'repercussions', 'repertoire', 'repetition', 'repetitions', 'repetitious', 'repetitive', 'rephrase', 'replace', 'replaced', 'replacement', 'replaces', 'replacing', 'replay', 'replayable', 'replayed', 'replaying', 'replays', 'replete', 'replica']\n",
      "Every 700th feature: ['00', 'affections', 'appropriately', 'barbra', 'blurbs', 'butchered', 'cheese', 'commitment', 'courts', 'deconstructed', 'disgraceful', 'dvds', 'eschews', 'fell', 'freezer', 'goriest', 'hauser', 'hungary', 'insinuate', 'juggle', 'leering', 'maelstrom', 'messiah', 'music', 'occasional', 'parking', 'pleasantville', 'pronunciation', 'recipient', 'reviews', 'sas', 'shea', 'sneers', 'steiger', 'swastika', 'thrusting', 'tvs', 'vampyre', 'westerns']\n"
     ]
    }
   ],
   "source": [
    "# 単語の抽出を改善できるか試す\n",
    "# 5つ以上の文書に現れたものだけをトークンとする\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(f\"X_train with min_df: {repr(X_train)}\")\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "print(f\"First 50 features: {feature_names[:50]}\")\n",
    "print(f\"Features 20010 to 20030: {feature_names[20010:20030]}\")\n",
    "print(f\"Every 700th feature: {feature_names[::700]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.88812\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"Best cross-validation score: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 318\n",
      "Every 10th stopword: ['behind', 'by', 'out', 'rather', 'around', 'everything', 'the', 'never', 'third', 'get', 'give', 'although', 'de', 'someone', 'full', 'him', 'amount', 'among', 'serious', 'other', 'hence', 'been', 'own', 'could', 'only', 'those', 'for', 'un', 'this', 'onto', 'beforehand', 'until']\n"
     ]
    }
   ],
   "source": [
    "# ストップワード\n",
    "# 役にたたない単語を取り除くもうひとつの手段として、あまりに頻出して役に立たない単語を捨てる方法がある\n",
    "# 1: 言語固有のストップワードリストを作っておく\n",
    "# 2: 頻度の高い単語を捨てる\n",
    "# skleanでは英語のストップワードリストをもっている\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(f\"Number of stop words: {len(ENGLISH_STOP_WORDS)}\")\n",
    "print(f\"Every 10th stopword: {list(ENGLISH_STOP_WORDS)[::10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with stp words: <25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2149958 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# 組み込みのstopwrodリストを使う\n",
    "vect = CountVectorizer(min_df=5, stop_words='english').fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(f\"X_train with stp words: {repr(X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.88296\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "# 今回のケースではストップワードによる恩恵は少ない。ただ、少しダけ処理を早くすることができる。\n",
    "print(f\"Best cross-validation score: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89392\n"
     ]
    }
   ],
   "source": [
    "# tf-idfを用いたデータのスケール変換\n",
    "# 特徴量がどの程度情報を持っていそうかに応じて、特徴量をスケールを変換する手法がある\n",
    "# 一般的な手法として tf-idfがある\n",
    "# 特定の文書にだけ頻繁に現れる単語に大きな重みを与え、コーパスの中の多数の文書に現れる単語にはあまり重みを与えない\n",
    "# つまり、特定の文書にあけ頻出し、他の文書にはあまり現れない単語は、その文書の内容をよく示しているのではないか、という発想\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None), LogisticRegression())\n",
    "\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]} \n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(f\"Best cross-validation score: {grid.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf: ['closest' 'staring' 'combine' ... 'rob' 'timon' 'titanic']\n",
      "Features with highest tfidf: ['coop' 'homer' 'dillinger' 'hackenstein' 'gadget' 'taker' 'macarthur'\n",
      " 'vargas' 'jesse' 'basket' 'dominick' 'the' 'victor' 'bridget' 'victoria'\n",
      " 'khouri' 'zizek' 'rob' 'timon' 'titanic']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "# 訓練データセットを変換\n",
    "X_train = vectorizer.transform(text_train)\n",
    "# それぞれの特徴量のデータセット中での最大値を見つける\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_byidf = max_value.argsort()\n",
    "# 特徴量名を取得\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "print(f\"Features with lowest tfidf: {feature_names[sorted_byidf[20:]]}\")\n",
    "print(f\"Features with highest tfidf: {feature_names[sorted_byidf[-20:]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf: ['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'have' 'one' 'be' 'film' 'are' 'you' 'all'\n",
      " 'at' 'an' 'by' 'so' 'from' 'like' 'who' 'they' 'there' 'if' 'his' 'out'\n",
      " 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'good' 'can' 'more' 'when'\n",
      " 'time' 'up' 'very' 'even' 'only' 'no' 'would' 'my' 'see' 'really' 'story'\n",
      " 'which' 'well' 'had' 'me' 'than' 'much' 'their' 'get' 'were' 'other'\n",
      " 'been' 'do' 'most' 'don' 'her' 'also' 'into' 'first' 'made' 'how' 'great'\n",
      " 'because' 'will' 'people' 'make' 'way' 'could' 'we' 'bad' 'after' 'any'\n",
      " 'too' 'then' 'them' 'she' 'watch' 'think' 'acting' 'movies' 'seen' 'its'\n",
      " 'him']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(f\"Features with lowest idf: {feature_names[sorted_by_idf[:100]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "mglearn.tools.visualize_coefficients(grid.best_estimator_.named_steps['logisticregression'].coef_, feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bards_words: ['The fool doth think he is wise,', 'but the wise man knows himself to be a fool']\n"
     ]
    }
   ],
   "source": [
    "# 1単語よりも大きい意味のBoW(n-gram)\n",
    "print(f\"bards_words: {bards_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary: ['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 1)).fit(bards_word)\n",
    "print(f\"Vocabulary size: {len(cv.vocabulary_)}\")\n",
    "print(f\"Vocabulary: {cv.get_feature_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 14\n",
      "Vocabulary: ['be fool', 'but the', 'doth think', 'fool doth', 'he is', 'himself to', 'is wise', 'knows himself', 'man knows', 'the fool', 'the wise', 'think he', 'to be', 'wise man']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_word)\n",
    "# 対象とするトークン列の長さを長くすると特徴量の数が増大し、特定的な特徴量となる\n",
    "# トークン列のアプリケーションはたいていの場合、長さを1にした方が良い\n",
    "print(f\"Vocabulary size: {len(cv.vocabulary_)}\")\n",
    "print(f\"Vocabulary: {cv.get_feature_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 39\n",
      "Vocabulary: ['be', 'be fool', 'but', 'but the', 'but the wise', 'doth', 'doth think', 'doth think he', 'fool', 'fool doth', 'fool doth think', 'he', 'he is', 'he is wise', 'himself', 'himself to', 'himself to be', 'is', 'is wise', 'knows', 'knows himself', 'knows himself to', 'man', 'man knows', 'man knows himself', 'the', 'the fool', 'the fool doth', 'the wise', 'the wise man', 'think', 'think he', 'think he is', 'to', 'to be', 'to be fool', 'wise', 'wise man', 'wise man knows']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_word)\n",
    "print(f\"Vocabulary size: {len(cv.vocabulary_)}\")\n",
    "print(f\"Vocabulary: {cv.get_feature_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グリッドサーチを使って、最良値を探ってみる\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "# グリッドが比較的大きい上、トリグラムが含まれているので\n",
    "# このグリッドサーチの実行にはかなり時間がかかｒる\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100], 'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print(f\"Best cross-validation score: {grid.best_score_}\")\n",
    "print(f\"Best parameters: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 個々の単語を語感を使って表現してあげる\n",
    "# 同じ単語を持つすべての単語を特定する必要がある\n",
    "# 単語の末尾につく特定の形を取り除くといったようばルールベースのヒューリスティックで行う場合＝＞語幹処理\n",
    "# 単語の文書での役割を考慮して行う場合＝＞見出し語化\n",
    "# 語幹処理で広く用いられるPorter stemmer をnltkパッケージから使う、見出し語化にはspacyパッケージを用いる\n",
    "# pip install spacy && python -m spacy download en\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "# spacyの英語モデルをロード\n",
    "en_nlp = spacy.load('en')\n",
    "# nltkのPorter stemmerのインスタンスを作成\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# spacyによる見出し語化とntlkによる語幹処理を比較する関数を定義\n",
    "def compare_normalization(doc):\n",
    "    # spacyで文書をトークン分割\n",
    "    doc_spacy = en_nlp(doc)\n",
    "    # spacyで見つけた見出し語を表示\n",
    "    print(f\"Lemmatization: {[token.lemma_ for token in doc_spacy]}\")\n",
    "    # Porter stemmerで見つけたトークンを表示\n",
    "    print(f\"Stemming: {[stemmer.stem(token.norm_.lower()) for token in doc_spacy]}\")\n",
    "\n",
    "compare_normalization(\"Our meeting today was worse than yesterday, I'm scared of meetings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 技術的詳細：CountVectorizerが用いている正規表現ベースの\n",
    "# トークン分割器を用いて、見出し語化だけにspacyを用いるのが望ましい\n",
    "# このため、en_nlp.tokenizer (spacyのトークン分割器)を正規表現ベースのトークン分割器に置き換えている\n",
    "import re\n",
    "# CountVectorizerで用いられているトークン分割用の正規表現\n",
    "# regexp used in CountVectorizer\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "\n",
    "# spacyの言語モデルを読み込み、トークン分割器を取り出す\n",
    "en_nlp = spacy.load('en')\n",
    "pld_tokenizer = en_nlp.tokenizer\n",
    "# トークン分割器を先ほどの正規表現で置き換える\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(regexp.findall(string))\n",
    "\n",
    "#spacyの文書処理パイプラインを持ちてカスタムトークン分割器をつくる\n",
    "# (正規表現を用いたトークン分割器を組み込んである)\n",
    "def custome_tokenizer(document):\n",
    "    do_spacy = en_nlp(document, entity=False, parse=False)\n",
    "    return [token.lemma_ for token in doc_spacy ]\n",
    "\n",
    "# CountVectorizerをカスタムトークン分割器を使って定義する\n",
    "lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 見出し語化を行う\n",
    "X_train_lemma = lemma_vect.fit_transform(text_train)\n",
    "print(f\"X_train_lemma.shape: {X_train_lemma.shape}\")\n",
    "\n",
    "# 比較のために標準のCountVectorizerでも変換\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(f\"X_train.shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 見出し語化の有効性を確認するために、データの1%だけを訓練データとし、残りをテストデータとして交差検証を行う\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "cv = StratifiedShuffleSplit(n_iter=5, test_size=0.99, train_size=0.01, random_state=0)\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\n",
    "# 標準のCountVectorizerを用いてグリッドサーチを実行\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"Best cross-validation score (standard CountVectorizer): f{grid.best_score_}\")\n",
    "# 見出し語化つきで、グリッドサーチを実行\n",
    "grid.fit(X_train_lemma, y_train)\n",
    "print(f\"Best cross-validation score(lemmatization): {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トピックモデリングと文書クラスタリング\n",
    "# それぞれの文書に対して1つのトピックを割り当てるタスクをまとめて呼ぶ言葉である"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA(Latent Dirichlet Allocation)\n",
    "# 同時に現れる頻度の高い単語の集合（トピック）を探す\n",
    "# 一般的な単語が解析に影響を与え過ぎないように、一般的な単語を取り除いたほうがよいとされる\n",
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from numpy as np\n",
    "import mglearn\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=10, learning_method='batch', max_iter=25, ramdom_state=0)\n",
    "\n",
    "# ここではモデルの構築と変換を一度に行う\n",
    "# 変換には時間がかかるが、同時に行うことで\n",
    "# 時間を節約することができる\n",
    "\n",
    "document_topic = lda.fit_transform(X)\n",
    "# それぞれの単語のそのトピックに対する重要性を格納した components_ 属性がある\n",
    "print(lda.components_.shape)\n",
    "\n",
    "# それぞれのトピックに対して、特徴量を昇順でソート\n",
    "# ソートを降順にするために[:, ::-1]\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "# vectorizerから特徴量名を取得\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# 最初の10トピックを表示\n",
    "mglearn.tools.print_topics(topics=range=(10), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# より高度なテキスト処理を行うにはspacy ntlk gensim などを使うことがおすすめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
